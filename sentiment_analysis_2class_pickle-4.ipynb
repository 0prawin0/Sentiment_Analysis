{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a93959ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re,string,unicodedata\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.metrics import accuracy_score ,roc_auc_score,f1_score,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from lightgbm import LGBMClassifier\n",
    "nltk.download('popular')\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1cc9722",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('train.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75aed267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment Time of Tweet Age of User  \\\n",
       "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
       "1                             Sooo SAD  negative          noon       21-30   \n",
       "2                          bullying me  negative         night       31-45   \n",
       "3                       leave me alone  negative       morning       46-60   \n",
       "4                        Sons of ****,  negative          noon       60-70   \n",
       "\n",
       "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
       "0  Afghanistan          38928346         652860.0               60  \n",
       "1      Albania           2877797          27400.0              105  \n",
       "2      Algeria          43851044        2381740.0               18  \n",
       "3      Andorra             77265            470.0              164  \n",
       "4       Angola          32866272        1246700.0               26  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()\n",
    "#first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "594fed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train[['text','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e3f7486",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37108a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27481 entries, 0 to 27480\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       27481 non-null  object\n",
      " 1   sentiment  27481 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 429.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30907290",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train[df_train['sentiment']!='neutral']\n",
    "df_train['sentiment']=df_train['sentiment'].map({'positive':1,'negative':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74f52af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_train['text']\n",
    "y=df_train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8d7f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner():\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if type(X) == str:\n",
    "            return self.clean_text(X)\n",
    "        else:\n",
    "            return [self.clean_text(text) for text in X]\n",
    "    def clean_text(self, text):\n",
    "        text = str(text).lower()  # Make text lowercase\n",
    "        text = re.sub('\\[.*?\\]', '', text)  # Remove any sequence of characters in square brackets\n",
    "        text = re.sub('https?://\\S+|www\\.\\S+', '', text)  # Remove links\n",
    "        text = re.sub('<.*?>+', '', text)  # Remove HTML tags\n",
    "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  # Remove punctuation\n",
    "        text = re.sub('\\n', '', text)  # Remove newline characters\n",
    "        text = re.sub('\\w*\\d\\w*', '', text)  # Remove words containing numbers\n",
    "        text = re.sub(r'[^a-z/A-Z/0-9/ ]', '', text)  # Remove special characters\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69fb4511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class stopwords():\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if type(X) == str:\n",
    "            return self.stopw(X)\n",
    "        else:\n",
    "            return [self.stopw(text) for text in X]\n",
    "    \n",
    "    def stopw(self,text):\n",
    "        from nltk.corpus import stopwords\n",
    "        stopwords = stopwords.words('english')\n",
    "        stopwords=stopwords+['s','m','u','im','ye','id','atg','na','ta','gon','wan']\n",
    "        text= ' '.join([x for x in text.split() if x not in stopwords])\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c70915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lemma:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if type(X) == str:\n",
    "            return [self.lemmatize(X)]\n",
    "        else:\n",
    "            return [self.lemmatize(text) for text in X]\n",
    "\n",
    "    def lemmatize(self, text):\n",
    "        tokens = word_tokenize(text)\n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        lemmatized_text = \" \".join(lemmatized_tokens)\n",
    "        return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20d0d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_array(X):\n",
    "    return X.toarray()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2fdbca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('cleaner', TextCleaner()),\n",
    "    ('stop_words_removal',stopwords()),\n",
    "    ('lemmatization',lemma())  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f90c83e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=TfidfVectorizer()\n",
    "X_tfidf=tf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f919f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb=MultinomialNB()\n",
    "mnb_model=mnb.fit(X_tfidf,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88fa1ffe-580e-4068-b21a-f18a9dbf584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('test.csv',encoding='latin-1')\n",
    "df_test=df_test[['text','sentiment']]\n",
    "df_test['text'] = df_test['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07dd683d-8e7b-4462-8a4b-6c6ad99ca354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4815 entries, 0 to 4814\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       4815 non-null   object\n",
      " 1   sentiment  3534 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 75.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f203eea1-75c6-47ae-b43c-4754b98bd56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.dropna(inplace=True,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5253140-64b0-46c7-b902-643cf85fcc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df_test[df_test['sentiment']!='neutral']\n",
    "df_test['sentiment']=df_test['sentiment'].map({'positive':1,'negative':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47b57fa8-155f-4092-aedc-d5801d185ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=df_test['text']\n",
    "y_test=df_test['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a3e0442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "pickle.dump(pipeline, gzip.open('Text_preprocessing.pkl', 'wb'))\n",
    "pickle.dump(tf,gzip.open('vectorizer.pkl','wb'))\n",
    "pickle.dump(mnb,gzip.open('Sentiment_detector.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f86523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_clean=pickle.load(gzip.open('Text_preprocessing.pkl','rb'))\n",
    "tf=pickle.load(gzip.open('vectorizer.pkl','rb'))\n",
    "model=pickle.load(gzip.open('Sentiment_detector.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8fd415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=txt_clean.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8f195ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=tf.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d39dbcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train=model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "813801d9-7c80-4b00-b89c-6fe3038eb298",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=txt_clean.fit_transform(X_test)\n",
    "\n",
    "X_test=tf.transform(X_test)\n",
    "\n",
    "pred_test=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "179e5a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9247692965837561"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y,pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "667d9f0a-08de-49b2-8d82-e8f4d15d10f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8726235741444867"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37e096d1-2d4d-4234-8e90-e946049b92f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9290612574194664"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y,pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb67e81a-b903-4d03-89b1-a3ab9e5a308e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8799283154121865"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
