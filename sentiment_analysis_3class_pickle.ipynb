{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a93959ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\naren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re,string,unicodedata\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.svm import SVC\n",
    "# from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.metrics import accuracy_score ,roc_auc_score,f1_score,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('popular')\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f1cc9722",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('train.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "75aed267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment Time of Tweet Age of User  \\\n",
       "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
       "1                             Sooo SAD  negative          noon       21-30   \n",
       "2                          bullying me  negative         night       31-45   \n",
       "3                       leave me alone  negative       morning       46-60   \n",
       "4                        Sons of ****,  negative          noon       60-70   \n",
       "\n",
       "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
       "0  Afghanistan          38928346         652860.0               60  \n",
       "1      Albania           2877797          27400.0              105  \n",
       "2      Algeria          43851044        2381740.0               18  \n",
       "3      Andorra             77265            470.0              164  \n",
       "4       Angola          32866272        1246700.0               26  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()\n",
    "#first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "594fed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train[['text','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3e3f7486",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "37108a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27481 entries, 0 to 27480\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       27481 non-null  object\n",
      " 1   sentiment  27481 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 429.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "30907290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train=df_train[df_train['sentiment']!='neutral']\n",
    "df_train['sentiment']=df_train['sentiment'].map({'positive':1,'negative':0,'neutral':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "74f52af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_train['text']\n",
    "y=df_train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d8d7f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner():\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if type(X) == str:\n",
    "            return self.clean_text(X)\n",
    "        else:\n",
    "            return [self.clean_text(text) for text in X]\n",
    "    def clean_text(self, text):\n",
    "        text = str(text).lower()  # Make text lowercase\n",
    "        text = re.sub('\\[.*?\\]', '', text)  # Remove any sequence of characters in square brackets\n",
    "        text = re.sub('https?://\\S+|www\\.\\S+', '', text)  # Remove links\n",
    "        text = re.sub('<.*?>+', '', text)  # Remove HTML tags\n",
    "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  # Remove punctuation\n",
    "        text = re.sub('\\n', '', text)  # Remove newline characters\n",
    "        text = re.sub('\\w*\\d\\w*', '', text)  # Remove words containing numbers\n",
    "        text = re.sub(r'[^a-z/A-Z/0-9/ ]', '', text)  # Remove special characters\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "69fb4511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class stopwords():\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if type(X) == str:\n",
    "            return self.stopw(X)\n",
    "        else:\n",
    "            return [self.stopw(text) for text in X]\n",
    "    \n",
    "    def stopw(self,text):\n",
    "        from nltk.corpus import stopwords\n",
    "        stopwords = stopwords.words('english')\n",
    "        stopwords=stopwords+['s','m','u','im','ye','id','atg','na','ta','gon','wan']\n",
    "        text= ' '.join([x for x in text.split() if x not in stopwords])\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "092651a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lemma:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if type(X) == str:\n",
    "            return [self.lemmatize(X)]\n",
    "        else:\n",
    "            return [self.lemmatize(text) for text in X]\n",
    "\n",
    "    def lemmatize(self, text):\n",
    "        tokens = word_tokenize(text)\n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        lemmatized_text = \" \".join(lemmatized_tokens)\n",
    "        return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "657bce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class lemma():\n",
    "    \n",
    "#     def __init__(self,lemma_model):\n",
    "#         self.lemma_model=lemma_model\n",
    "        \n",
    "#     def fit(self,X,y=None):\n",
    "#         return self\n",
    "    \n",
    "#     def transform(self,X):\n",
    "#         if type(X) == str:\n",
    "#             return [self.lemmatise(X)]\n",
    "#         else:\n",
    "#             return [self.lemmatise(text) for text in X]\n",
    "    \n",
    "#     def lemmatise(self,text):\n",
    "#         return \" \".join([token.lemma_ for token in self.lemma_model(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "20d0d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_array(X):\n",
    "    return X.toarray()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b2fdbca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('cleaner', TextCleaner()),\n",
    "    ('stop_words_removal',stopwords()),\n",
    "    ('lemmatization',lemma())\n",
    "#     ('TF-IDF-fit',TfidfVectorizer()),\n",
    "    #('TF-IDF-Transform',FunctionTransformer(to_array,accept_sparse=True))\n",
    "    #('toarray',FunctionTransformer(to_array,accept_sparse=True)),\n",
    "#     ('model',MultinomialNB())\n",
    "    \n",
    "        \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cee7c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe=pipeline.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dd917d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=TfidfVectorizer()\n",
    "X_tfidf=tf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f919f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb=MultinomialNB()\n",
    "mnb_model=mnb.fit(X_tfidf,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4a734825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with gzip.open('path/to/your_model.pkl.gz', 'wb') as file:\n",
    "#     pickle.dump(your_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7a3e0442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "pickle.dump(pipeline, gzip.open('Text_preprocessing_3.pkl', 'wb'))\n",
    "pickle.dump(tf,gzip.open('vectorizer_3.pkl','wb'))\n",
    "pickle.dump(mnb,gzip.open('Sentiment_detector_3.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f86523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_clean=pickle.load(gzip.open('Text_preprocessing_3.pkl','rb'))\n",
    "tf=pickle.load(gzip.open('vectorizer_3.pkl','rb'))\n",
    "model=pickle.load(gzip.open('Sentiment_detector_3.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e0fc9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=txt_clean.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2eae3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b73a48ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab46d495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.767948764600997"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3b250a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('test.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d1f678f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df_test[['text','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4fe4180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text'] = df_test['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "185c46a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4815 entries, 0 to 4814\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       4815 non-null   object\n",
      " 1   sentiment  3534 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 75.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1f789935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test=df_test[df_test['sentiment']!='neutral']\n",
    "df_test['sentiment']=df_test['sentiment'].map({'positive':1,'negative':0,'neutral':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a5d42ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=df_train['text']\n",
    "y_test=df_train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ef7407fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=txt_clean.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "481f721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=tf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "67ed46e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6770b473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7667793489048255"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,pred_test,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae9ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
